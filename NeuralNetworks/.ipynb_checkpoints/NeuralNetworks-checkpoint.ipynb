{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous part of this exercise, I implemented multi-class logistic regression to recognize handwritten digits. However, logistic regression cannot form more complex hypotheses as it is only a linear classifier. In this par, I will implement a neural network to recognize handwritten digits using the same training set as before. The neural network will be able to represent complex models that form non-linear hypotheses. For this week, I will be using parameters from a neural network that we have already trained. Your goal is to implement the feedforward propagation algorithm to use our weights for prediction. In next weekâ€™s exercise, you will write the backpropagation algorithm for learning the neural network parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still, we use data set from Machine Learning Course by Stanford, let's look at the given weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = sio.loadmat('ex3data1')#Note, this is a dictionary\n",
    "wghts = sio.loadmat('ex3weights')\n",
    "X = data['X'] #Features\n",
    "y = data['y'] #Digit class\n",
    "weights = []\n",
    "Theta1 = wghts['Theta1'] #weights for the hidden layer\n",
    "Theta2 = wghts['Theta2'] #weights for the output layer\n",
    "weights.append(Theta1)\n",
    "weights.append(Theta2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X size (5000, 400)\n",
      "Theta1 size (25, 401)\n",
      "Theta2 size (10, 26)\n"
     ]
    }
   ],
   "source": [
    "print 'X size', X.shape\n",
    "print 'Theta1 size', Theta1.shape\n",
    "print 'Theta2 size', Theta2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Shuffle the data and split at random\n",
    "y[y==10] = 0#Map 10 into 0\n",
    "#Transform y into arrays of integers directly\n",
    "y = y.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward Propagation and Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above all, we need to computer feedforward propagation given input and weights, it is quite easy to do so. And also it is needed for backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define sigmoid function\n",
    "def sigmoid(Z):\n",
    "    return 1/(1 + np.exp(-Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Calculate the accuracy\n",
    "def accuracy(y, y_test):\n",
    "    '''Calculate Accuracy'''\n",
    "    return np.mean(y==y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Add oncstants to the original input\n",
    "#Create theta with theta0\n",
    "def addOnes(X):\n",
    "    if X is None:\n",
    "        print 'Input Null!'\n",
    "        return None, None\n",
    "    dim = X.shape\n",
    "    #If X has multi variables\n",
    "    if dim>1:\n",
    "        feature_num = X.shape[1]\n",
    "        X = np.insert(X, 0, 1, axis=1)\n",
    "        theta = np.zeros(feature_num+1)\n",
    "        return X, theta\n",
    "    #If X only has one variable\n",
    "    else:\n",
    "        temp_X = np.ones([len(X), 2])\n",
    "        temp_X[:, 0] = X\n",
    "        theta = np.zeros(2)\n",
    "        return temp_X, theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create a function to do predictions\n",
    "def modelPredict(test, label_num=10):\n",
    "    '''Make predictions'''\n",
    "    if len(test.shape) < 2:\n",
    "        print 'The input has too few dimensions'\n",
    "        return None\n",
    "    #Select the class which has largest probability\n",
    "    #Note, 10 is mapped into 0\n",
    "    predictions = [(z.argmax()+1)%label_num for z in test]    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X: input, 2 dimension numpy array\n",
    "#weights: weights for each layer, a list\n",
    "def feedforwardNeuralNetwork(X, weights):\n",
    "    '''Calculate feedforward propagation output'''\n",
    "    ######Deal with extreme cases###\n",
    "    if X is None or weights is None:\n",
    "        print 'Invalid Input!'\n",
    "        return None\n",
    "    dim = X.shape\n",
    "    if len(dim) < 2:\n",
    "        print 'X has too less variables'\n",
    "        return None\n",
    "    #####Define variables###########\n",
    "    X, _ = addOnes(X)\n",
    "    sample_num = len(X)\n",
    "    feature_num = X.shape[1]\n",
    "    layer_num = len(weights)\n",
    "    output_h = []#Output for each layer\n",
    "    output_h.append(X)#The first layer is equal to input X\n",
    "    input_z = []#Input for each layer, starts from the second layer\n",
    "    #####Make alculations for each layer, except the input layer\n",
    "    for i in range(layer_num):\n",
    "        z = np.dot(output_h[i], np.transpose(weights[i]))\n",
    "        h = sigmoid(z)\n",
    "        if i < layer_num - 1:\n",
    "            h, _ = addOnes(h)\n",
    "        output_h.append(h)\n",
    "        input_z.append(z)\n",
    "    return h, output_h, input_z    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Make predictions based on input and layer weights\n",
    "def neuralNetworkPredict(X, weights):\n",
    "    h, _, _ = feedforwardNeuralNetwork(X, weights)\n",
    "    pred = modelPredict(h)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = neuralNetworkPredict(X, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97519999999999996"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(pred==y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized Cost Function\n",
    "\n",
    "We need to define cost function for a neural network, in order to prevent against overfitting, we take regularization into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.056882279686836297"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(sum(Theta2[:,1:]**2))*1/2/5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#h: probability of y, size is sample number * label number\n",
    "#y: label, size is sample number * label number, one hot encoder\n",
    "#weights: weight for each layer, a list\n",
    "#label_num: label number, 10 for digits\n",
    "def costFuncWithReg(h, y, weights, lambda1=0.01):\n",
    "    '''Calculate the cost of neural network'''\n",
    "    if h is None or y is None or weights is None:\n",
    "        print 'Invalid Input!'\n",
    "        return None\n",
    "    sample_num = len(y)#Length of y\n",
    "    layer_num = len(weights)\n",
    "    #Cost of errors\n",
    "    total = -sum(sum(y*np.log(h)))/sample_num - sum(sum((1-y)*np.log(1-h)))/sample_num\n",
    "    #Cost of regularization\n",
    "    for wgt in weights:\n",
    "        total +=  sum(sum(wgt[:,1:]**2))*lambda1/2/sample_num    \n",
    "    return total    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Map each label into a vector\n",
    "def mapLabelToMatrix(y, label_num=10):\n",
    "    if y is None:\n",
    "        print 'Invalid Input!'\n",
    "        return None\n",
    "    sample_num = len(y)\n",
    "    y_mat = np.zeros([sample_num, label_num])\n",
    "    for i in range(1, label_num+1):\n",
    "        vec = (y==(i%label_num))#1,2,3,4,5,6,7,8,9,0\n",
    "        y_mat[:, i-1] = vec\n",
    "    return y_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define cost function, X stands for input matrix\n",
    "#y stands for the labels\n",
    "#weights stands for the parameters between layers\n",
    "#lambda1 stands for the regularization coefficient\n",
    "def calculateCost(X, y, weights, lambda1=0.01):\n",
    "    '''Calculate the errors of neural network'''\n",
    "    ###############Deal with unusual inputs###########\n",
    "    if X is None or y is None:\n",
    "        print 'Empty Input For Cost!'\n",
    "        return None\n",
    "\n",
    "    ##############Map y into matrix####################\n",
    "    y_mat = mapLabelToMatrix(y, 10)\n",
    "    \n",
    "    #############Compute the cost######################\n",
    "    h, _ = feedforwardNeuralNetwork(X, weights)\n",
    "    total = costFuncWithReg(h, y_mat, weights, lambda1=lambda1)\n",
    "    \n",
    "    return total     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38376985909092359"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculateCost(X, y, weights, lambda1=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's move to the most complex part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initialize weights\n",
    "def initializeWeights(n_in, n_out, epsilon=0.12):\n",
    "    W = np.random.randn(n_out, n_in+1) * 2 * epsilon - epsilon\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Calculate the gradient of a sigmoid function\n",
    "def sigmoidGradient(z):\n",
    "    h = sigmoid(z)\n",
    "    return h * (1-h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 10)\n"
     ]
    }
   ],
   "source": [
    "#Train a neural network model through backpropagation\n",
    "np.random.seed(111)\n",
    "input_layer_size = X.shape[1]#Input layer size\n",
    "label_num = 10#Output class number\n",
    "weights = []#Weights list, weight for each layer\n",
    "weights_grad = []#Weights gradient\n",
    "layer_size = [input_layer_size, 25, label_num]#Node number for each layer\n",
    "layer_num = len(layer_size) #Number of layer\n",
    "#Initialize weights for each layer\n",
    "for i in range(layer_num-1):\n",
    "    wgt = initializeWeights(layer_size[i], layer_size[i+1])\n",
    "    weights.append(wgt)\n",
    "#Initialize the gradient of weights\n",
    "for i in range(layer_num-1):\n",
    "    wgt_grad = np.zeros([layer_size[i+1], layer_size[i]+1])\n",
    "    weights_grad.append(wgt_grad)\n",
    "\n",
    "h, output_h, input_z = feedforwardNeuralNetwork(X, weights)\n",
    "#Transform y into matrix\n",
    "y_mat = mapLabelToMatrix(y, label_num)\n",
    "\n",
    "#Backpropagation\n",
    "errors = []#errors for each layer\n",
    "#Error for the output layer\n",
    "error = h - y_mat\n",
    "errors.append(error)\n",
    "print error.shape\n",
    "#Error for the hidden layer, exclude the input and output error\n",
    "for i in range(layer_num-2):\n",
    "    #Start from the last layer\n",
    "    layer_index = layer_num - 2 - i\n",
    "    wgt = weights[layer_index]\n",
    "    #Remove delta 0\n",
    "    error = np.dot(errors[i], wgt)[:,1:] * sigmoidGradient(input_z[i])\n",
    "    errors.append(error)\n",
    "    \n",
    "#Calculate accumulated gradients\n",
    "delta = []\n",
    "for i in range(len(errors)):\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 25)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_z[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function range in module __builtin__:\n",
      "\n",
      "range(...)\n",
      "    range(stop) -> list of integers\n",
      "    range(start, stop[, step]) -> list of integers\n",
      "    \n",
      "    Return a list containing an arithmetic progression of integers.\n",
      "    range(i, j) returns [i, i+1, i+2, ..., j-1]; start (!) defaults to 0.\n",
      "    When step is given, it specifies the increment (or decrement).\n",
      "    For example, range(4) returns [0, 1, 2, 3].  The end point is omitted!\n",
      "    These are exactly the valid indices for a list of 4 elements.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(1,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
