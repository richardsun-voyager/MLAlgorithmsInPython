{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous part of this exercise, I implemented multi-class logistic regression to recognize handwritten digits. However, logistic regression cannot form more complex hypotheses as it is only a linear classifier. In this par, I will implement a neural network to recognize handwritten digits using the same training set as before. The neural network will be able to represent complex models that form non-linear hypotheses. For this week, I will be using parameters from a neural network that we have already trained. Your goal is to implement the feedforward propagation algorithm to use our weights for prediction. In next weekâ€™s exercise, you will write the backpropagation algorithm for learning the neural network parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still, we use data set from Machine Learning Course by Stanford, let's look at the given weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = sio.loadmat('ex3data1')#Note, this is a dictionary\n",
    "wghts = sio.loadmat('ex3weights')\n",
    "X = data['X'] #Features\n",
    "y = data['y'] #Digit class\n",
    "weights = []\n",
    "Theta1 = wghts['Theta1'] #weights for the hidden layer\n",
    "Theta2 = wghts['Theta2'] #weights for the output layer\n",
    "weights.append(Theta1)\n",
    "weights.append(Theta2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X size (5000, 400)\n",
      "Theta1 size (25, 401)\n",
      "Theta2 size (10, 26)\n"
     ]
    }
   ],
   "source": [
    "print 'X size', X.shape\n",
    "print 'Theta1 size', Theta1.shape\n",
    "print 'Theta2 size', Theta2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Shuffle the data and split at random\n",
    "y[y==10] = 0#Map 10 into 0\n",
    "#Transform y into arrays of integers directly\n",
    "y = y.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward Propagation and Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above all, we need to computer feedforward propagation given input and weights, it is quite easy to do so. And also it is needed for backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define sigmoid function\n",
    "def sigmoid(Z):\n",
    "    return 1/(1 + np.exp(-Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Calculate the accuracy\n",
    "def accuracy(y, y_test):\n",
    "    '''Calculate Accuracy'''\n",
    "    return np.mean(y==y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Add oncstants to the original input\n",
    "#Create theta with theta0\n",
    "def addOnes(X):\n",
    "    if X is None:\n",
    "        print 'Input Null!'\n",
    "        return None, None\n",
    "    dim = X.shape\n",
    "    #If X has multi variables\n",
    "    if dim>1:\n",
    "        feature_num = X.shape[1]\n",
    "        X = np.insert(X, 0, 1, axis=1)\n",
    "        theta = np.zeros(feature_num+1)\n",
    "        return X, theta\n",
    "    #If X only has one variable\n",
    "    else:\n",
    "        temp_X = np.ones([len(X), 2])\n",
    "        temp_X[:, 0] = X\n",
    "        theta = np.zeros(2)\n",
    "        return temp_X, theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create a function to do predictions\n",
    "def modelPredict(test, label_num=10):\n",
    "    '''Make predictions'''\n",
    "    if len(test.shape) < 2:\n",
    "        print 'The input has too few dimensions'\n",
    "        return None\n",
    "    #Select the class which has largest probability\n",
    "    #Note, 10 is mapped into 0\n",
    "    predictions = [(z.argmax()+1)%label_num for z in test]    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X: input, 2 dimension numpy array\n",
    "#weights: weights for each layer, a list\n",
    "def feedforwardNeuralNetwork(X, weights):\n",
    "    '''Calculate feedforward propagation output'''\n",
    "    ######Deal with extreme cases###\n",
    "    if X is None or weights is None:\n",
    "        print 'Invalid Input!'\n",
    "        return None\n",
    "    dim = X.shape\n",
    "    if len(dim) < 2:\n",
    "        print 'X has too less variables'\n",
    "        return None\n",
    "    #####Define variables###########\n",
    "    X, _ = addOnes(X)\n",
    "    sample_num = len(X)\n",
    "    feature_num = X.shape[1]\n",
    "    layer_num = len(weights)\n",
    "    output_h = []#Output for each layer\n",
    "    output_h.append(X)#The first layer is equal to input X\n",
    "    input_z = []#Input for each layer, starts from the second layer\n",
    "    #####Make alculations for each layer, except the input layer\n",
    "    for i in range(layer_num):\n",
    "        z = np.dot(output_h[i], np.transpose(weights[i]))\n",
    "        h = sigmoid(z)\n",
    "        if i < layer_num - 1:\n",
    "            h, _ = addOnes(h)\n",
    "        output_h.append(h)\n",
    "        input_z.append(z)\n",
    "    return h, output_h, input_z    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Make predictions based on input and layer weights\n",
    "def neuralNetworkPredict(X, weights):\n",
    "    h, _, _ = feedforwardNeuralNetwork(X, weights)\n",
    "    pred = modelPredict(h)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = neuralNetworkPredict(X, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97519999999999996"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(pred==y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The given parameters works well in classifying the 10 digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized Cost Function\n",
    "\n",
    "We need to define cost function for a neural network, in order to prevent against overfitting, we take regularization into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#h: probability of y, size is sample number * label number\n",
    "#y: label, size is sample number * label number, one hot encoder\n",
    "#weights: weight for each layer, a list\n",
    "#label_num: label number, 10 for digits\n",
    "def costFuncWithReg(h, y, weights, lambda1=0.01):\n",
    "    '''Calculate the cost of neural network'''\n",
    "    if h is None or y is None or weights is None:\n",
    "        print 'Invalid Input!'\n",
    "        return None\n",
    "    sample_num = len(y)#Length of y\n",
    "    layer_num = len(weights)\n",
    "    #Cost of errors\n",
    "    total = -sum(sum(y*np.log(h)))/sample_num - sum(sum((1-y)*np.log(1-h)))/sample_num\n",
    "    #Cost of regularization\n",
    "    for wgt in weights:\n",
    "        total +=  sum(sum(wgt[:,1:]**2))*lambda1/2/sample_num    \n",
    "    return total    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Map each label into a vector\n",
    "def mapLabelToMatrix(y, label_num=10):\n",
    "    if y is None:\n",
    "        print 'Invalid Input!'\n",
    "        return None\n",
    "    sample_num = len(y)\n",
    "    y_mat = np.zeros([sample_num, label_num])\n",
    "    for i in range(1, label_num+1):\n",
    "        vec = (y==(i%label_num))#1,2,3,4,5,6,7,8,9,0\n",
    "        y_mat[:, i-1] = vec#The first column stands for 1, the last for 10\n",
    "    return y_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define cost function, X stands for input matrix\n",
    "#y stands for the labels\n",
    "#weights stands for the parameters between layers\n",
    "#lambda1 stands for the regularization coefficient\n",
    "def calculateCost(X, y_mat, weights, lambda1=0.01):\n",
    "    '''Calculate the errors of neural network'''\n",
    "    ###############Deal with unusual inputs###########\n",
    "    if X is None or y is None:\n",
    "        print 'Empty Input For Cost!'\n",
    "        return None\n",
    "\n",
    "    ##############Map y into matrix####################\n",
    "    #y_mat = mapLabelToMatrix(y, 10)\n",
    "    \n",
    "    #############Compute the cost######################\n",
    "    h, _, _ = feedforwardNeuralNetwork(X, weights)\n",
    "    total = costFuncWithReg(h, y_mat, weights, lambda1=lambda1)\n",
    "    \n",
    "    return total     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38376985909092359"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Map y into matrix: sample num * label num\n",
    "y_mat = mapLabelToMatrix(y, 10)\n",
    "calculateCost(X, y_mat, weights, lambda1=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's move to the most complex part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initialize weights\n",
    "def initializeWeights(n_in, n_out, epsilon=0.12):\n",
    "    W = np.random.randn(n_out, n_in+1) * 2 * epsilon - epsilon\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Calculate the gradient of a sigmoid function\n",
    "def sigmoidGradient(z):\n",
    "    h = sigmoid(z)\n",
    "    return h * (1-h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Train a neural network model through backpropagation\n",
    "np.random.seed(111)\n",
    "sample_num = len(y)\n",
    "input_layer_size = X.shape[1]#Input layer size\n",
    "label_num = 10#Output class number\n",
    "weights = []#Weights list, weight for each layer\n",
    "weights_grad = []#Weights gradient\n",
    "layer_size = [input_layer_size, 25, label_num]#Node number for each layer\n",
    "layer_num = len(layer_size) #Number of layer\n",
    "iter_num = 10000 #Iteration number\n",
    "alpha = 0.003 #Gradient descending learning rate\n",
    "lambda1 = 1 #Regularization coefficient\n",
    "#Transform y into matrix\n",
    "y_mat = mapLabelToMatrix(y, label_num)\n",
    "\n",
    "\n",
    "#Initialize weights for each layer\n",
    "for i in range(layer_num-1):\n",
    "    wgt = initializeWeights(layer_size[i], layer_size[i+1])\n",
    "    weights.append(wgt)\n",
    "#Initialize the gradient of weights\n",
    "for i in range(layer_num-1):\n",
    "    wgt_grad = np.zeros([layer_size[i+1], layer_size[i]+1])\n",
    "    weights_grad.append(wgt_grad)\n",
    "   \n",
    "J = []#cost for each iteration    \n",
    "#Learning from the errors\n",
    "#Using gradient descending method\n",
    "for _ in range(iter_num):\n",
    "    h, output_h, input_z = feedforwardNeuralNetwork(X, weights)\n",
    "    cost = costFuncWithReg(h, y_mat, weights, lambda1=lambda1)\n",
    "    J.append(cost)\n",
    "    #Backpropagation\n",
    "    errors = []#errors for each layer\n",
    "    #Error for the output layer\n",
    "    error = h - y_mat\n",
    "    errors.append(error)\n",
    "    #print error.shape\n",
    "    #Error for the hidden layer, exclude the input and output error\n",
    "    for i in range(layer_num-2):\n",
    "        #Start from the last layer\n",
    "        layer_index = layer_num - 2 - i\n",
    "        wgt = weights[layer_index]\n",
    "        #Remove delta 0, otherwise there will be an error\n",
    "        error = np.dot(errors[i], wgt)[:,1:] * sigmoidGradient(input_z[i])\n",
    "        errors.append(error)\n",
    "    #errors.reverse()#Reverse the order, the hidden layer comes first\n",
    "    #Calculate accumulated gradients\n",
    "    Delta = []\n",
    "    for i in range(len(errors)):\n",
    "        layer_index = layer_num - 2 - i\n",
    "        delta =np.dot(np.transpose(errors[i]), output_h[layer_index])\n",
    "        Delta.append(delta)   \n",
    "    #Add regularization\n",
    "    Delta.reverse()\n",
    "    grad = Delta\n",
    "    for i in range(len(grad)):\n",
    "        grad[i] = grad[i] + np.insert(weights[i][:,1:], 0, 0, axis=1) * lambda1\n",
    "        weights[i] = weights[i] - alpha * grad[i]/sample_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0xe313230>]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": [
       "iVBORw0KGgoAAAANSUhEUgAAAX8AAAEACAYAAABbMHZzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\n",
       "AAALEgAACxIB0t1+/AAAGi5JREFUeJzt3X+0HWV97/H3hxwChECOCA2SRBIhVLAUAgopleZoqZVc\n",
       "TLuUtth70YVLTf3RYq1V8McV73XRX7IQ2gJZSl0UwRRDxXAblR/1IFYSCIQYCSHhV01AktQ0gSQq\n",
       "Br73j3kOmWzOj33Omb1n9p7Pa61ZZ8/Ms2e+M1n5Ps9+ZuYZRQRmZlYv+5UdgJmZtZ+Tv5lZDTn5\n",
       "m5nVkJO/mVkNOfmbmdWQk7+ZWQ2NmPwl/aqkVblph6Q/G6TclZI2SFotaU5rwjUzsyL0jFQgIh4B\n",
       "5gBI2g94CvhGvoyk+cCxETFb0unA1cDc4sM1M7MijLbb5yzgsYjY2LB8AXAdQESsAHolTS0gPjMz\n",
       "a4HRJv/zgBsHWT4NyFcIm4DpYw3KzMxaq+nkL2ki8Dbg60MVaZj3uBFmZhU1Yp9/ztnA/RGxdZB1\n",
       "TwEzcvPT07KXSHJlYGY2BhHR2Lget9Ek/3cCXxti3VLgw8BiSXOB7RGxubFQKw6gE0m6JCIuKTuO\n",
       "KvC52MvnYi+fi71a1XBuKvlLOpjsYu/7cssWAkTEoohYJmm+pEeBXcAFrQjWzMyK0VTyj4hdwOEN\n",
       "yxY1zH+4wLjMzKyF/IRvOfrLDqBC+ssOoEL6yw6gQvrLDqDbqV0vc5EU7vM3MxudVuVOt/zNzGrI\n",
       "yd/MrIac/M3MasjJ38yshpz8zcxqyMnfzKyGnPzNzGrIyd/MrIac/M3MasjJ38yshpz8zcxqyMnf\n",
       "zKyGnPzNzGrIyd/MrIac/M3MasjJ38yshpz8zcxqqKnkL6lX0hJJD0taK2luw/o+STskrUrTp1sT\n",
       "rpmZFaGpF7gDVwDLIuJcST3AwYOUuSsiFhQXmpmZtcqIyV/SFODMiHg3QETsAXYMVnTkbaEI2vPS\n",
       "YDMzG1Iz3T6zgK2SviLpAUlfkjSpoUwAZ0haLWmZpBOG2NZR44rWzMwK0Uzy7wFOAa6KiFOAXcBF\n",
       "DWUeAGZExEnA3wO3DLGt48YaqJmZFaeZPv9NwKaIuC/NL6Eh+UfEc7nP35J0laTDImLbvpua/xfS\n",
       "t+almf6I6B9r4GZm3UhSH9DX6v2MmPwj4hlJGyUdFxHrgbOAh/JlJE0FtkRESDoN0MsTP8Cy9RFc\n",
       "UkjkZmZdKDWK+wfmJX22Fftp9m6fPwVukDQReAx4j6SFABGxCDgX+ICkPcBu4LwhtuNuHzOzClBE\n",
       "e26+kRQQGyJcAZiZNUtSRMSId1OOVruf8H21xP5t3qeZmTVod/LfRHbrqJmZlajdyX897vc3Mytd\n",
       "Gcn/V9u8TzMza9Du5P8IbvmbmZXOLX8zsxpqd/JfB7y2zfs0M7MG7U7+TwMHSryyzfs1M7Octib/\n",
       "NJzzWmCoUT/NzKwNyniNo5O/mVnJykr+rythv2ZmlpSR/B/CLX8zs1K528fMrIbKSP6bgMkSryhh\n",
       "32ZmRgnJP93x8zBu/ZuZlaaMlj+468fMrFRlJf+H8B0/ZmalccvfzKyGnPzNzGqoqeQvqVfSEkkP\n",
       "S1orae4gZa6UtEHSaklzRtjkj4Feid6xBG1mZuPTbMv/CmBZRBwP/DrZ3TovkTQfODYiZgPvB64e\n",
       "bmMRvEjW73/iqCM2M7NxGzH5S5oCnBkR/wQQEXsiYkdDsQXAdWn9CqBX0tQRNr2arCIxM7M2a6bl\n",
       "PwvYKukrkh6Q9CVJkxrKTAM25uY3AdNH2O5q4KTmQzUzs6L0NFnmFODDEXGfpC8CFwH/u6GcGuaj\n",
       "cUOSLtk798Gfwj86+ZuZ5UjqA/pavp+Il+XoxkCOBO6JiFlp/o3ARRFxTq7MNUB/RCxO8+uAeRGx\n",
       "OVcmIkJ755lC9nKXQyN4ocBjMjPrGo25sygjdvtExDPARkkDL14/i+xibd5S4F0A6U6g7fnEP/h2\n",
       "2QFsAY4ZbdBmZjY+zXT7APwpcIOkicBjwHskLQSIiEURsUzSfEmPAruAC5rc7kC///pRxm1mZuMw\n",
       "YrdPYTsa5KeLxOeACRF8ui1BmJl1mNK6fVrsh/h2TzOztis7+ft2TzOzEpSd/B8HDvOLXczM2qvU\n",
       "5J+GeViDh3kwM2urslv+kHX9nFx2EGZmdVKF5L8SOLXsIMzM6qQKyf9+4PVlB2FmViel3uefLWd/\n",
       "YDswNYKdbQnGzKxDdOt9/kTwS7KLviO9AMbMzApSevJPVuKuHzOztqlK8r8fX/Q1M2ubqiR/t/zN\n",
       "zNqo9Au+2Tp6yC76HhXBs20JyMysA3TtBV+ACPaQPex1StmxmJnVQSWSf+KHvczM2qRKyf9+4A1l\n",
       "B2FmVgdVSv4rgNPLDsLMrA6qlPzXA4dKvKrsQMzMul1lkn8EASwHfqPsWMzMul1TyV/Sk5J+KGmV\n",
       "pHsHWd8naUdav0rSWN/J+wOc/M3MWq6nyXIB9EXEtmHK3BURC8YZzz3A/x3nNszMbASj6fYZ6SGD\n",
       "Ih5CuBc4WWJiAdsyM7MhNJv8A7hD0kpJ7xti/RmSVktaJumEsQSThnRej0f4NDNrqWa7fX4zIn4i\n",
       "6QjgdknrIuLu3PoHgBkRsVvS2cAtwHGNG5F0SW62PyL6B9nXPcAZZLd+mpnViqQ+oK/l+xnt2D6S\n",
       "PgvsjIjLhinzBHBq/hpBs+NTSJwPvC2CPxxVYGZmXai0sX0kTZJ0SPp8MPAWspev5MtMlaT0+TSy\n",
       "SmW4i8PD8R0/ZmYt1ky3z1TgGym39wA3RMRtkhYCRMQi4FzgA5L2ALuB88YR0+PARImjI/jPcWzH\n",
       "zMyGUIkhnV9elq8Dt0bwzy0Oy8ys0rp6SOdB9APzyg7CzKxbVTX534WTv5lZy1Q1+a8FeiWmlx2I\n",
       "mVk3qmTyj+BF4Hu49W9m1hKVTP5JP07+ZmYtUeXkfxdteMrNzKyOqpz81wCH++UuZmbFq2zyz/X7\n",
       "95UciplZ16ls8k/uBM4qOwgzs25T9eR/G/AWqZB3BZiZWVL15L8eeBF4bdmBmJl1k0on//RS99vI\n",
       "RhI1M7OCVDr5J07+ZmYFq+Sonvt+j8OAJ4EjIvhF4YGZmVVY3Ub1fEkE24CHyV7taGZmBah88k++\n",
       "g7t+zMwK0ynJ/zbgrWUHYWbWLTol+S8HZkjMKDsQM7Nu0BHJP4I9wLeAc8qOxcysGzSV/CU9KemH\n",
       "klZJuneIMldK2iBptaQ5xYYJwFJgQQu2a2ZWOz1NlgugLyK2DbZS0nzg2IiYLel04GpgbkExDvgO\n",
       "cK3E5Ah2FrxtM7NaGU23z3D3mS4ArgOIiBVAr6Sp4wmsUQTPAiuA3ylyu2ZmddRs8g/gDkkrJb1v\n",
       "kPXTgI25+U3QkvfvuuvHzKwAzXb7/GZE/ETSEcDtktZFxN0NZRp/Gbzs0WFJl+Rm+yOiv+lIM7cC\n",
       "n5GYEMELo/yumVnlSeqjDe8xGfXwDpI+C+yMiMtyy64hS+aL0/w6YF5EbM6VKeQRZYkfAh+M4Pvj\n",
       "3ZaZWdWVNryDpEmSDkmfDyZ70nZNQ7GlwLtSmbnA9nziL9gS4A9atG0zs1oYseUvaRbwjTTbA9wQ\n",
       "EX8laSFARCxK5f6B7CncXcAFEfFAw3aKavkfD9wBzEivejQz61qtavlXflTPwbfFGuAD7voxs25X\n",
       "21E9h/B13PVjZjZmndryd9ePmdWCW/45ETwMbMNj/JuZjUlHJv/kJuCPyg7CzKwTdWS3T7Y9jgHu\n",
       "AaZH8HxR2zUzqxJ3+zSI4DHgEfySFzOzUevY5J/8M+nhMjMza17Hdvtk26QXeBJ4TXrRu5lZV3G3\n",
       "zyAi2A58G1/4NTMblY5O/om7fszMRqkbkv9twNESJ5QdiJlZp+j45J9e7n4tsLDsWMzMOkVHX/Dd\n",
       "u21eDawiG+5hdyv2YWZWBl/wHUYEPwZ+gC/8mpk1pSuSf3IN8CdlB2Fm1gm6Kfl/GzhS4pSyAzEz\n",
       "q7quSf7phe6LgA+VHYuZWdV1xQXfvfvgcGADcHwEz7RyX2Zm7eALvk2I4L+Axbj1b2Y2rKaSv6QJ\n",
       "klZJunWQdX2SdqT1qyR9uvgwR+VyYKHEpJLjMDOrrJ4my10IrAUOGWL9XRGxoJiQxieC9RL3AO8G\n",
       "ri47HjOzKhqx5S9pOjAf+DIwVL9TS/vyx+Ay4M8lJpQdiJlZFTXT7XM58Jcw5IvSAzhD0mpJyyRV\n",
       "YYydu8ne8fuOsgMxM6uiYbt9JJ0DbImIVZL6hij2ADAjInZLOhu4BThuiO1dkpvtj4j+UUfchAhC\n",
       "4v8AfyuxJGLIisvMrFJSru1r+X6Gu9VT0qXA+cAe4EDgUODmiBhyCGVJTwCnRsS2huUtv9Vz3/0h\n",
       "4F7gbyJY0q79mpkVqVW5s+n7/CXNAz4WEW9rWD6V7NdBSDoNuCkiZg7y/bYm/2yfnANcCpzs1r+Z\n",
       "daKq3OcfKZiFkgaGUD4XWCPpQeCLwHkFxjde/wY8D/x+2YGYmVVJVz3hO/h+OQf4a+CkNASEmVnH\n",
       "qErLvxP9G9mdP+8uOxAzs6ro+pZ/tm9OB24GjvPLXsysk7jlPw4RrCB72ctHyo7FzKwKatHyz/bP\n",
       "scByshE/t5YVh5nZaJR+q+e4d1Ry8s9i4HLgkAjeW2YcZmbNcvIvJAamAA8Db49geZmxmJk1w33+\n",
       "BYhgB9k4RVd50Dczq7NaJf/kRuBZYOFIBc3MulWtun0GSLwO+C4wJ4Knyo7HzGwo7vYpUAQPAVcB\n",
       "i9IAcGZmtVLL5J9cCswgG7XUzKxWatntM0BiDvAdslE/ny47HjOzRu72aYEIVpG95/daqd7nwszq\n",
       "xQkPPg+8Ag/9YGY1UutunwESs4AVwP+I4L6y4zEzG+BunxaK4AngQ8BiiUPLjsfMrNXc8s+RuAY4\n",
       "AvgDv/bRzKrALf/2uBA4CvhU2YGYmbWSk39OBL8A3g68X+L3yo7HzKxVmkr+kiZIWiXp1iHWXylp\n",
       "g6TVkuYUG2J7RfAT4B3AlyV+rex4zMxaodmW/4XAWuBlFwgkzQeOjYjZwPvJ7pvvaBHcS3bMyyRm\n",
       "lB2PmVnRRkz+kqYD84Evw6Dj4CwArgOIiBVAr6SpRQZZhghuBK4EviXxirLjMTMrUjMt/8vJxsAf\n",
       "6u6XacDG3PwmYPo446qKy4DbgG9KHFR2MGZmRekZbqWkc4AtEbFKUt9wRRvmB71/VNIludn+iOhv\n",
       "IsbSRBASHwOuB5ZIvD1dFDYza4mUa/tavp/h7vOXdCnZqJd7gAOBQ4GbI+JduTLXkCXyxWl+HTAv\n",
       "IjY3bKvy9/kPRaKH7CUwk4B3uAIws3Yp5T7/iPhkRMyIiFnAecC/5xN/shR4VwpyLrC9MfF3ugj2\n",
       "AP8T+DnZL4ADSg7JzGxcRnuffwBIWihpIUBELAMel/QosAj4YLEhVkMEvwTeCTxPdg1gcskhmZmN\n",
       "mYd3GKXUBbQIOJFsILitJYdkZl3MwztUROoCei9wO/B9iaNLDsnMbNSc/McggojgU8A/Aj+QOKPs\n",
       "mMzMRsPJfxwiuBJ4H3CLxHvKjsfMrFnu8y+AxGvJ7npaBnw8gudLDsnMuoT7/CssgnXA6cAxwN0S\n",
       "ryk5JDOzYTn5FySC/yYb5+hrwHKJPyw5JDOzIbnbpwUkXg8sBr4P/HmqGMzMRs3dPh0kgpXAHGAn\n",
       "sEbibSWHZGa2D7f8W0xiHnAtsAL4aARdNfSFmbWWW/4dKoK7gJOAp4EfSXxEYv+SwzKzmnPLv40k\n",
       "jgeuIHtJ/IUR3FlySGZWca3KnU7+bSYh4PfJXhSzHvhUBPeXG5WZVZW7fbpEGhriG/DSg2G3Snw9\n",
       "/SowM2sLJ/+SRPB8BFcBxwIrge9J3CRxasmhmVkNOPmXLILdEfwN8BpgOdk4QbdL/HbqIjIzK5z7\n",
       "/CtGYiLwx8DHgReAq4CvRvBcqYGZWSl8wbdmUqv/zcAH0t/FwNURrCk1MDNrKyf/GpOYRjZ09HuB\n",
       "LcD1wNcieKbUwMys5UpL/pIOBO4CDgAmAt+MiIsbyvQB3wQeT4tujojPN5Rx8h8niQlAH3A+8Htk\n",
       "1wi+Cvy/CHaUGJqZtUipLX9JkyJit6QessHKPhYR38+t7wM+GhELhtmGk3+BJA4mqwD+GPgt4D+A\n",
       "fwWWeggJs+5R6n3+EbE7fZwITAC2DVLMib2NItgVwY0RnANMA75Cdm3gEYm7JS6WOEXyHV1m9nJN\n",
       "JQZJ+0l6ENgMfDci1jYUCeAMSaslLZN0QtGB2tAieC6CmyJ4JzAVuBQ4ErgR+InEVyXOlziy1EDN\n",
       "rDJGdcFX0hTgO8BFEdGfW34I8ELqGjobuCIijmv4rrt9SiAxE3gL8Ltkvwy2AHen6XvAkxG056q/\n",
       "mY1aZe72kfQZ4GcR8YVhyjwBnBoR23LLAvhcrlh/vgKx1ksXjH8NOJPsOsGZZM8S3E025PRKYFUE\n",
       "u0oL0qzm0jXUvtyiz5Z1t8/hwJ6I2C7pILKW/+ci4s5cmanAlogISacBN0XEzIbtuOVfMelZgmPI\n",
       "KoE3AK8HXgc8BtxHVhmsBB6KYPdQ2zGz1inzVs8TgevIrg/sB1wfEX8naSFARCyS9CGyh5H2ALvJ\n",
       "7vxZ3o4DsGJJHACcSFYRvAE4FTgOeApYk6Yfpb+PRrCnpFDNaqEy3T5j3pGTf8dKL5+ZTdZldGJu\n",
       "ehWwgWxo6n2mCH5aTrRm3cXJ3ypHYjLZr4LBphfIKoINwJNp+s/0d2MEz7c9YLMO5ORvHSNdSzic\n",
       "rBKYDcxM09Hp71Fkdx0NVAZPAhvJXnX5NFkX09YIXmhn3GZV5ORvXUOih+zBtJm5aTpZpTAt/e0l\n",
       "e65koDLIVwzPkFUeW8gqCf+KsK7l5G+1ki48H8m+FcLA56nAr6TpCGAXeyuDxmlr+vtfZE+mb4vg\n",
       "Z+08FrPxcPI3G0QavqKXrBL4lUGmI8gqi1cCh6W/L5Iqgtz000GW5aftwM4IXmzToZkBTv5mhUjX\n",
       "Iw4iqwgap1cOs7w3fe9ZYAdZZbBjDJ9/7ieqbTSc/M1Klq5VHEpWEUxJU/5z4/xgn/dj30phB1mF\n",
       "8izwXO7zSPM/cyVSD07+Zl0gXcvIVwaH5qZDGuaHWzaRl1cOo61AngWei+CXrT1qGw8nfzN7SXrw\n",
       "bqBiaKbSGK7MLxm6chiuAnkO2Jn+PkdWkfiJ74I5+ZtZ4XLXQMZSiRzSME0GnmdvZbBPxdDE8sEq\n",
       "k9o/6+Hkb2aVlqtIGiuFgYphsOXDrTsY+AVjqDSGWL6zEysTJ38zq5VUmUyi+MpkJ9mzIQPTziE+\n",
       "j2Zdyy7AO/mbmY1DeibkILJKYGCaPMTnkeYb1x3A0BXDaOZ355alz9rl5G9mVkHpRUlFVCqTGv4e\n",
       "DDrQyd/MrGZalTubeoG7mZl1Fyd/M7MacvI3M6shJ38zsxoaNvlLOlDSCkkPSlor6a+GKHelpA2S\n",
       "Vkua05pQzcysKMMm/4j4OfCmiDgZ+HXgTZLemC8jaT5wbETMBt4PXN2qYLuFpL6yY6gKn4u9fC72\n",
       "8rlovRG7fSJid/o4EZhA9mKLvAXAdansCqBX0tQig+xCfWUHUCF9ZQdQIX1lB1AhfWUH0O1GTP6S\n",
       "9pP0INn7VL8bEWsbikwje/n2gE1k72M1M7OKaqbl/2Lq9pkO/NYQP8caH0DwSybMzCpsVE/4SvoM\n",
       "8LOI+EJu2TVAf0QsTvPrgHkRsbnhu64QzMzGoBVP+PYMt1LS4cCeiNgu6SDgd4DPNRRbCnwYWCxp\n",
       "LrC9MfFDa4I3M7OxGTb5A68CrpO0H1kX0fURcaekhQARsSgilkmaL+lRspHoLmhtyGZmNl5tG9jN\n",
       "zMyqoy1P+Ep6q6R16UGwT7Rjn+0kaYak70p6SNKPJP1ZWn6YpNslrZd0m6Te3HcuTudjnaS35Jaf\n",
       "KmlNWndFGcdTBEkTJK2SdGuar+W5kNQraYmkh9ODkqfX+FxcnP6PrJF0o6QD6nIuJP2TpM2S1uSW\n",
       "FXbs6Vz+S1q+XNLRIwYVES2dyJ4NeBSYCewPPAgc3+r9tnMCjgROTp8nA48AxwN/C3w8Lf8E8Nfp\n",
       "8wnpPOyfzsuj7P0Vdi9wWvq8DHhr2cc3xnPyUeAGYGmar+W5IHsG5j3pcw8wpY7nIh3P48ABaf5f\n",
       "gHfX5VwAZwJzgDW5ZYUdO/BB4Kr0+Y+AxSPG1IaD/g3g27n5i4CLyv7HaPEx3wKcBawDpqZlRwLr\n",
       "0ueLgU/kyn8bmEt2jeXh3PLzgGvKPp4xHP904A7gTcCtaVntzkVK9I8PsryO5+IwskbRK8gqwVvJ\n",
       "biCpzblIiTyf/As79lTm9PS5B9g6Ujzt6PYZ7CGwaW3YbykkzSSr4VeQ/cMO3Pm0GRh48vkosvMw\n",
       "YOCcNC5/is48V5cDfwm8mFtWx3MxC9gq6SuSHpD0JUkHU8NzERHbgMuAHwNPk90VeDs1PBc5RR77\n",
       "S3k2IvYAOyQdNtzO25H8a3NFWdJk4Gbgwoh4Lr8usiq568+FpHOALRGxipc//AfU51yQtcBOIfs5\n",
       "fgrZ3XAX5QvU5VxIOgb4CFnr9yhgsqT/lS9Tl3MxmDKOvR3J/ylgRm5+BvvWXl1B0v5kif/6iLgl\n",
       "Ld4s6ci0/lXAlrS88ZxMJzsnT7Hv0BjT07JOcgawQNITwNeAN0u6nnqei03Apoi4L80vIasMnqnh\n",
       "uXg98IOI+Glqmf4rWZdwHc/FgCL+T2zKfefVaVs9wJT0a2tI7Uj+K4HZkmZKmkh2MWJpG/bbNpIE\n",
       "XAusjYgv5lYtJbuoRfp7S275eZImSpoFzAbujYhngGfTHSECzs99pyNExCcjYkZEzCLrk/z3iDif\n",
       "ep6LZ4CNko5Li84CHiLr767VuSDr354r6aB0DGcBa6nnuRhQxP+Jbw6yrXOBO0fce5sudJxNdrHn\n",
       "UeDisi+8tOD43kjWv/0gsCpNbyW7yHUHsB64DejNfeeT6XysA343t/xUYE1ad2XZxzbO8zKPvXf7\n",
       "1PJcACcB9wGryVq7U2p8Lj5OVvmtIbsLav+6nAuyX8FPA8+T9c1fUOSxAwcANwEbgOXAzJFi8kNe\n",
       "ZmY15Nc4mpnVkJO/mVkNOfmbmdWQk7+ZWQ05+ZuZ1ZCTv5lZDTn5m5nVkJO/mVkN/X8CBpkKJhz9\n",
       "mwAAAABJRU5ErkJggg==\n"
      ],
      "text/plain": [
       "<matplotlib.figure.Figure at 0xde1c630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = neuralNetworkPredict(X, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24779999999999999"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(pred==y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
